{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd55d96-ae17-4fa5-89e8-88f847456a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the Breast Cancer dataset\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "# Create a DataFrame with features\n",
    "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "# Add target variable to DataFrame\n",
    "df['target'] = breast_cancer.target\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc60e4-e496-4bae-9c7d-0a3283fac96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb675150-17e3-4ccd-9648-56538515d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting panda\n",
      "  Downloading panda-0.3.1.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from panda) (68.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from panda) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->panda) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->panda) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->panda) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->panda) (2024.2.2)\n",
      "Building wheels for collected packages: panda\n",
      "  Building wheel for panda (setup.py): started\n",
      "  Building wheel for panda (setup.py): finished with status 'done'\n",
      "  Created wheel for panda: filename=panda-0.3.1-py3-none-any.whl size=7247 sha256=33c4bc20571a98de08429c1383969b95b80f44005a25f0185a5b0a720e955f94\n",
      "  Stored in directory: c:\\users\\asadi\\appdata\\local\\pip\\cache\\wheels\\e9\\b8\\cc\\e2f18ad77320b551f1b7435014844a9cc3e9d6185cb93a3a31\n",
      "Successfully built panda\n",
      "Installing collected packages: panda\n",
      "Successfully installed panda-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44069626-38e1-4ea4-826f-fefd46542fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 11ms/step - loss: 0.5484 - accuracy: 0.7714 - val_loss: 0.3633 - val_accuracy: 0.9474\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2689 - accuracy: 0.9538 - val_loss: 0.1430 - val_accuracy: 0.9649\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9692 - val_loss: 0.0904 - val_accuracy: 0.9561\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9714 - val_loss: 0.0659 - val_accuracy: 0.9737\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9824 - val_loss: 0.0754 - val_accuracy: 0.9561\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.9824 - val_loss: 0.0868 - val_accuracy: 0.9561\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9934 - val_loss: 0.0662 - val_accuracy: 0.9825\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.9934 - val_loss: 0.0709 - val_accuracy: 0.9825\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9956 - val_loss: 0.0823 - val_accuracy: 0.9825\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9956 - val_loss: 0.0790 - val_accuracy: 0.9825\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9934 - val_loss: 0.0875 - val_accuracy: 0.9825\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9978 - val_loss: 0.0783 - val_accuracy: 0.9737\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.0919 - val_accuracy: 0.9737\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9978 - val_loss: 0.0894 - val_accuracy: 0.9737\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0980 - val_accuracy: 0.9737\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.9978 - val_loss: 0.0903 - val_accuracy: 0.9737\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 0.9934 - val_loss: 0.0967 - val_accuracy: 0.9737\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9978 - val_loss: 0.1066 - val_accuracy: 0.9737\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.1048 - val_accuracy: 0.9737\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.1122 - val_accuracy: 0.9737\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 0.1160 - val_accuracy: 0.9737\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 0.1158 - val_accuracy: 0.9737\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 0.9978 - val_loss: 0.1209 - val_accuracy: 0.9737\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.9978 - val_loss: 0.1223 - val_accuracy: 0.9737\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 0.9978 - val_loss: 0.1268 - val_accuracy: 0.9737\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.9978 - val_loss: 0.1277 - val_accuracy: 0.9737\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 0.9978 - val_loss: 0.1276 - val_accuracy: 0.9737\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.9978 - val_loss: 0.1341 - val_accuracy: 0.9737\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.9978 - val_loss: 0.1334 - val_accuracy: 0.9737\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0026 - accuracy: 0.9978 - val_loss: 0.1345 - val_accuracy: 0.9737\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.9978 - val_loss: 0.1357 - val_accuracy: 0.9737\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.9978 - val_loss: 0.1387 - val_accuracy: 0.9737\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9978 - val_loss: 0.1400 - val_accuracy: 0.9737\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9978 - val_loss: 0.1408 - val_accuracy: 0.9737\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.9978 - val_loss: 0.1428 - val_accuracy: 0.9737\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9737\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1440 - val_accuracy: 0.9737\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9737\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 9.4447e-04 - accuracy: 1.0000 - val_loss: 0.1470 - val_accuracy: 0.9737\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 8.5281e-04 - accuracy: 1.0000 - val_loss: 0.1501 - val_accuracy: 0.9737\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.3662e-04 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9737\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.4770e-04 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9737\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2152e-04 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9737\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.3926e-04 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9737\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1463e-04 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9737\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.3086e-04 - accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9737\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.9257e-04 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9737\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 2.6043e-04 - accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9737\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.2783e-04 - accuracy: 1.0000 - val_loss: 0.1622 - val_accuracy: 0.9737\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.0578e-04 - accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9737\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.8094e-04 - accuracy: 1.0000 - val_loss: 0.1670 - val_accuracy: 0.9737\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.6120e-04 - accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9737\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.5458e-04 - accuracy: 1.0000 - val_loss: 0.1692 - val_accuracy: 0.9737\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3698e-04 - accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9737\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2242e-04 - accuracy: 1.0000 - val_loss: 0.1730 - val_accuracy: 0.9737\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1230e-04 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9737\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0373e-04 - accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9737\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 9.7047e-05 - accuracy: 1.0000 - val_loss: 0.1753 - val_accuracy: 0.9737\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 9.0602e-05 - accuracy: 1.0000 - val_loss: 0.1769 - val_accuracy: 0.9737\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 8.5607e-05 - accuracy: 1.0000 - val_loss: 0.1784 - val_accuracy: 0.9737\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 8.0596e-05 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9737\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.5727e-05 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9737\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7.2771e-05 - accuracy: 1.0000 - val_loss: 0.1804 - val_accuracy: 0.9737\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.8783e-05 - accuracy: 1.0000 - val_loss: 0.1809 - val_accuracy: 0.9737\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.4541e-05 - accuracy: 1.0000 - val_loss: 0.1826 - val_accuracy: 0.9737\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 6.1086e-05 - accuracy: 1.0000 - val_loss: 0.1827 - val_accuracy: 0.9737\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.8541e-05 - accuracy: 1.0000 - val_loss: 0.1834 - val_accuracy: 0.9737\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.6245e-05 - accuracy: 1.0000 - val_loss: 0.1842 - val_accuracy: 0.9737\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3226e-05 - accuracy: 1.0000 - val_loss: 0.1846 - val_accuracy: 0.9737\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.0979e-05 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9737\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.8699e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9737\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.7084e-05 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9737\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.5048e-05 - accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 0.9737\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.3810e-05 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9737\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1469e-05 - accuracy: 1.0000 - val_loss: 0.1889 - val_accuracy: 0.9737\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9935e-05 - accuracy: 1.0000 - val_loss: 0.1893 - val_accuracy: 0.9737\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8187e-05 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9737\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.7050e-05 - accuracy: 1.0000 - val_loss: 0.1905 - val_accuracy: 0.9737\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.5660e-05 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 0.9737\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.4410e-05 - accuracy: 1.0000 - val_loss: 0.1915 - val_accuracy: 0.9737\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.2962e-05 - accuracy: 1.0000 - val_loss: 0.1922 - val_accuracy: 0.9737\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.1841e-05 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 0.9737\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.0705e-05 - accuracy: 1.0000 - val_loss: 0.1936 - val_accuracy: 0.9737\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 3.0234e-05 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9737\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.9483e-05 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9737\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.1070e-05 - accuracy: 1.0000 - val_loss: 0.1982 - val_accuracy: 0.9737\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.9136e-05 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9737\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.6201e-05 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9737\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.4872e-05 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9737\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.3965e-05 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9737\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.3209e-05 - accuracy: 1.0000 - val_loss: 0.1974 - val_accuracy: 0.9737\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.2479e-05 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9737\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.1799e-05 - accuracy: 1.0000 - val_loss: 0.1983 - val_accuracy: 0.9737\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.1275e-05 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9737\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.0609e-05 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9737\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.9908e-05 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9737\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.9269e-05 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9737\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.8924e-05 - accuracy: 1.0000 - val_loss: 0.2002 - val_accuracy: 0.9737\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.8263e-05 - accuracy: 1.0000 - val_loss: 0.2010 - val_accuracy: 0.9737\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.7771e-05 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9737\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9737\n",
      "Accuracy on the Data:  97.36841917037964\n",
      "Loss Value : 20.14886438846588\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Load the Breast Cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "# Create a DataFrame with features\n",
    "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "# Add target variable to DataFrame\n",
    "df['target'] = breast_cancer.target\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n",
    "\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "# Add the input layer\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "# Add one more hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add another hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# Add another hidden layer\n",
    "model.add(Dense(16, activation='relu'))\n",
    "# Add another hidden layer\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# Add the output layer\n",
    "model.add(Dense(1, activation='sigmoid')) # Sigmoid for binary classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32,validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Accuracy on the Data: \", (accuracy*100))\n",
    "print(\"Loss Value :\", (loss*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c46ba7-5975-4749-8940-2667bcef8785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 26.1374\n",
      "Mean Squared Error on Test Data: 26.137353897094727\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()   \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Mean Squared Error on Test Data:\", loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c681c92-13bd-4a96-b6fa-652a9c5e975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 18.8953\n",
      "Mean Squared Error on Test Data: 18.895252227783203\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()   \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Mean Squared Error on Test Data:\", loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32069b5f-ae6d-44fc-8073-56e88b78db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 12.6778\n",
      "Mean Squared Error on Test Data: 12.677781105041504\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()   \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=300, batch_size=32, verbose=0)\n",
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Mean Squared Error on Test Data:\", loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46099956-04d6-44f5-b003-dfcdc73a40d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'california_housing' from 'tensorflow.keras.datasets' (C:\\Users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\api\\_v2\\keras\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m california_housing\n\u001b[0;32m      7\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m california_housing\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      9\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'california_housing' from 'tensorflow.keras.datasets' (C:\\Users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\api\\_v2\\keras\\datasets\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import california_housing\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = california_housing.load_data()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Mean Squared Error on Test Data:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e10eed9-bb5f-483b-b1fe-4811d708bb56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_california_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Choose the dataset (uncomment the desired one)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Option 1: Boston Housing (default)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Option 2: California Housing (requires tensorflow-datasets)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_california_housing\u001b[49m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Standardize features (works for both datasets)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fetch_california_housing' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Choose the dataset (uncomment the desired one)\n",
    "# Option 1: Boston Housing (default)\n",
    "# (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Option 2: California Housing (requires tensorflow-datasets)\n",
    "(X_train, y_train), (X_test, y_test) = fetch_california_housing.load_data()\n",
    "\n",
    "# Standardize features (works for both datasets)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... rest of your code for building and training the model\n",
    "\n",
    "\n",
    "# ... rest of your code for building and training the model\n",
    "\n",
    "\n",
    "def build_model(num_hidden_layers, neurons_per_layer):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "  for _ in range(num_hidden_layers - 1):  # Add hidden layers (excluding the first)\n",
    "    model.add(Dense(neurons_per_layer, activation='relu'))\n",
    "  model.add(Dense(1))  # Output layer (1 neuron for regression)\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')  # Regression task\n",
    "  return model\n",
    "\n",
    "# Experiment with different network architectures\n",
    "num_hidden_layers_list = [1, 2, 3]  # Experiment with different numbers of hidden layers\n",
    "neurons_per_layer_list = [8, 16, 32, 64]  # Experiment with different neurons per layer\n",
    "\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "  for neurons_per_layer in neurons_per_layer_list:\n",
    "    model = build_model(num_hidden_layers, neurons_per_layer)\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    loss = model.evaluate(X_test_scaled, y_test)\n",
    "    print(f\"Mean Squared Error (Test) - Layers: {num_hidden_layers}, Neurons: {neurons_per_layer}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b598a-192e-4415-855c-eafc24282077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3285b1c6-9b66-417a-8caf-c2cc572ec7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting array-record (from tensorflow-datasets)\n",
      "  Downloading array_record-0.4.0-py38-none-any.whl.metadata (502 bytes)\n",
      "Collecting click (from tensorflow-datasets)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath]>=0.9.0->tensorflow-datasets)\n",
      "  Downloading etils-1.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (1.24.3)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (4.25.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 434.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow-datasets) (6.1.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\asadi\\anaconda3\\envs\\tf\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting absl-py (from tensorflow-datasets)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl.metadata (699 bytes)\n",
      "Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.1/5.4 MB 1.7 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.1/5.4 MB 901.1 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/5.4 MB 1.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.3/5.4 MB 1.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.4/5.4 MB 1.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.6/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.6/5.4 MB 1.6 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.8/5.4 MB 2.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.2/5.4 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.5/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.4 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.0/5.4 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.2/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.0/5.4 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.5/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.5/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.5/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.3/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.4/5.4 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.6/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.9/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.1/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.4/126.4 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading array_record-0.4.0-py38-none-any.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/3.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/3.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.5/3.0 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.5/3.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.8/3.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.9/3.0 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.0/3.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.1/3.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.2/3.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.2/3.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.4/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.5/3.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.7/3.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.8/3.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.8/3.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.8/3.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.8/3.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.8/3.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.8/3.0 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading dm_tree-0.1.8-cp38-cp38-win_amd64.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.4/101.4 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.5/126.5 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 0.0/904.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 163.8/904.4 kB 5.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 194.6/904.4 kB 2.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 225.3/904.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 286.7/904.4 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 327.7/904.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 593.9/904.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 675.8/904.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 706.6/904.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 798.7/904.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 904.4/904.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.2 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 153.6/229.2 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 229.2/229.2 kB 3.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21545 sha256=ee82df0bac77e9eb03ff7ef5f448264fc90e5e4f673fa81fe05bc040e40fca85\n",
      "  Stored in directory: c:\\users\\asadi\\appdata\\local\\pip\\cache\\wheels\\54\\aa\\01\\724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, protobuf, promise, etils, click, absl-py, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "Successfully installed absl-py-1.4.0 array-record-0.4.0 click-8.1.7 dm-tree-0.1.8 etils-1.3.0 googleapis-common-protos-1.63.1 promise-2.3 protobuf-3.20.3 tensorflow-datasets-4.9.2 tensorflow-metadata-1.14.0 toml-0.10.2 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\asadi\\anaconda3\\envs\\tf\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be5cd699-7e53-4281-b5df-46005c999c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Choose the dataset (uncomment the desired one)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Option 1: Boston Housing (default)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Option 2: California Housing (requires tensorflow-datasets)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m fetch_california_housing(return_X_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Standardize features (works for both datasets)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Choose the dataset (uncomment the desired one)\n",
    "# Option 1: Boston Housing (default)\n",
    "# (X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Option 2: California Housing (requires tensorflow-datasets)\n",
    "(X_train, y_train), (X_test, y_test) = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "\n",
    "# Standardize features (works for both datasets)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... rest of your code for building and training the model\n",
    "\n",
    "\n",
    "# ... rest of your code for building and training the model\n",
    "\n",
    "\n",
    "def build_model(num_hidden_layers, neurons_per_layer):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "  for _ in range(num_hidden_layers - 1):  # Add hidden layers (excluding the first)\n",
    "    model.add(Dense(neurons_per_layer, activation='relu'))\n",
    "  model.add(Dense(1))  # Output layer (1 neuron for regression)\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')  # Regression task\n",
    "  return model\n",
    "\n",
    "# Experiment with different network architectures\n",
    "num_hidden_layers_list = [1, 2, 3]  # Experiment with different numbers of hidden layers\n",
    "neurons_per_layer_list = [8, 16, 32, 64]  # Experiment with different neurons per layer\n",
    "\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "  for neurons_per_layer in neurons_per_layer_list:\n",
    "    model = build_model(num_hidden_layers, neurons_per_layer)\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    loss = model.evaluate(X_test_scaled, y_test)\n",
    "    print(f\"Mean Squared Error (Test) - Layers: {num_hidden_layers}, Neurons: {neurons_per_layer}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346b3a93-0e37-499e-81c4-c94abf907952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 33.9527\n",
      "Mean Squared Error on Test Data: 33.952659606933594\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Model compilation\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Model evaluation\n",
    "loss = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Mean Squared Error on Test Data:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
